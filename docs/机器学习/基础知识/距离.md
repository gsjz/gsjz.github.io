# 距离

有一种理解是对于所谓 $L_p$ norm（闵可夫斯基距离）

$$
||(x_1,\cdots,x_n)||_p = (\sum |x_i|^p)^{\frac{1}{p}}
$$

$L_1$ 即曼哈顿距离，$L_2$ 即欧几里得距离。

$L_{\infty}$ 即切比雪夫距离（等价于 $\max_i\{x_i\}$）。

## batch normalization

对于一个 mini-batch

$$
x = \{x^{(1)},\cdots, x^{(m)}\},\quad x^{(i)} \in \mathbb R^d 
$$

其中 batch size 为 $m$，特征维度为 $d$。

对于每个维度，我们应分别作 normalization。所以不妨认为 $d=1$，省略一下下标的书写。

首先计算这个 mini-batch 内部的均值和方差

$$
\mu_B = \frac{1}{m}\sum\limits_{i=1}^{m} x^{(i)}
$$

$$
\sigma_B^2 = \frac{1}{m}\sum\limits_{i=1}^{m}(x^{(i)} - \mu_B)^2
$$

然后根据这两个参数对这个 mini-batch 内部每一个样本都做归一化

$$
\hat{x}^{(i)} = \frac{x^{(i)} - \mu_B}{\sqrt{\sigma_B^2 + \varepsilon}}
$$

这里 $\varepsilon$ 是一个小量，主要是考虑到 $\sigma_{B}$ 可能为 $0$，所以稍微防一下。

这么做完之后就能让这一维的均值为 $0$ 方差为 $1$。

---

这样的动机在于，可以在计算某种距离时，每一维的权重都是一致的。


